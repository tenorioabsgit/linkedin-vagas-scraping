# ğŸ“Œ Carregar bibliotecas necessÃ¡rias
library(chromote)
library(rvest)
library(dplyr)
library(stringr)
library(tidyverse)
library(parsedate)
library(ggplot2)
library(readr)
library(lubridate)
library(crayon)  # Adicionando pacote para cores no console
# ğŸ“Œ Criar uma nova sessÃ£o do Chromote
b <- ChromoteSession$new()
# ğŸ“Œ FunÃ§Ã£o para capturar vagas no LinkedIn com filtro de perÃ­odo e descriÃ§Ã£o da vaga
scrape_linkedin_jobs <- function(keyword, pages = 3, periodo = "r172800") {
message(blue$bold(paste("Iniciando raspagem para:", keyword)))  # Mensagem colorida de status
base_url <- "https://www.linkedin.com/jobs/search/?"
query <- paste0("keywords=", URLencode(keyword),
"&location=Brazil",
"&f_TPR=", periodo)
full_url <- paste0(base_url, query)
b$Page$navigate(full_url)
Sys.sleep(5)  # Tempo para carregar a pÃ¡gina
job_list <- list()
for (i in 1:pages) {
message(blue$bold(paste("Processando pÃ¡gina", i, "de", pages)))  # Mensagem colorida
page_source <- b$Runtime$evaluate("document.documentElement.outerHTML")$result$value
html <- read_html(page_source)
titles <- html %>% html_nodes(".base-search-card__title") %>% html_text(trim = TRUE)
companies <- html %>% html_nodes(".base-search-card__subtitle") %>% html_text(trim = TRUE)
locations <- html %>% html_nodes(".job-search-card__location") %>% html_text(trim = TRUE)
links <- html %>% html_nodes(".base-card__full-link") %>% html_attr("href")
dates <- html %>% html_nodes(".job-search-card__listdate") %>% html_text(trim = TRUE)
# Garantir que todas as colunas tenham o mesmo tamanho
max_length <- max(length(titles), length(companies), length(locations), length(links), length(dates))
titles <- c(titles, rep(NA, max_length - length(titles)))
companies <- c(companies, rep(NA, max_length - length(companies)))
locations <- c(locations, rep(NA, max_length - length(locations)))
links <- c(links, rep(NA, max_length - length(links)))
dates <- c(dates, rep(NA, max_length - length(dates)))
# Converter texto das datas para formato legÃ­vel
dates <- dates %>%
str_replace_all("hÃ¡ ", "") %>%
str_replace_all("dia", "dias") %>%
str_replace_all("um", "1") %>%
str_extract("\\d+") %>%
as.numeric() %>%
replace_na(0) %>%
{Sys.Date() - .}
# ğŸ”¹ Coletar descriÃ§Ã£o de cada vaga
descriptions <- map_chr(links, function(url) {
if (!is.na(url)) {
message(green$bold(paste("Coletando descriÃ§Ã£o da vaga em:", url)))  # Mensagem colorida
b$Page$navigate(url)
Sys.sleep(3)
page_source <- b$Runtime$evaluate("document.documentElement.outerHTML")$result$value
job_html <- read_html(page_source)
job_html %>%
html_nodes(".description__text") %>%
html_text(trim = TRUE) %>%
paste(collapse = " ")
} else {
NA
}
})
descriptions <- c(descriptions, rep(NA, max_length - length(descriptions)))
# Criar dataframe e adicionar a palavra-chave utilizada
job_data <- tibble(
Titulo = titles,
Empresa = companies,
Localizacao = locations,
Link = links,
Data_Publicacao = dates,
Descricao = descriptions,
Palavra_Chave = keyword  # Adicionando coluna com termo da busca
)
job_list <- append(job_list, list(job_data))
}
return(bind_rows(job_list))
}
# ğŸ“Œ Lista de palavras-chave para busca
keywords <- c("Data Scientist")
# ğŸ“Œ Coletar vagas para cada palavra-chave (Ãšltimos 60 dias)
vagas_total <- map_dfr(keywords, ~scrape_linkedin_jobs(.x, pages = 3, periodo = "r86400"))
# ğŸ“Œ Aplicar Regex para detectar ferramentas mencionadas
ferramentas_regex <- paste(c(
"Python", "R", "SQL", "Scala", "Java", "Julia", "C++",
"Pandas", "NumPy", "Scikit-Learn", "TensorFlow", "PyTorch", "Keras",
"AWS", "Azure", "Google Cloud", "GCP", "BigQuery", "Snowflake", "Databricks",
"Power BI", "Tableau", "Looker", "Google Data Studio", "Qlik Sense", "Metabase"
), collapse = "|")
vagas_total <- vagas_total %>%
mutate(Ferramentas = str_extract_all(str_to_lower(Descricao), ferramentas_regex))
# ğŸ“Œ Criar ranking das ferramentas mais citadas
ranking_ferramentas <- vagas_total %>%
unnest(Ferramentas) %>%
count(Ferramentas, sort = TRUE)
# ğŸ“Œ Salvar os resultados
write_csv(ranking_ferramentas, "ranking_ferramentas.csv")
print(green("âœ… Ranking de ferramentas salvo como 'ranking_ferramentas.csv'"))
# ğŸ“Œ Fechar sessÃ£o Chromote
b$close()
# ğŸ“Œ Carregar bibliotecas necessÃ¡rias
library(chromote)
library(rvest)
library(dplyr)
library(stringr)
library(tidyverse)
library(parsedate)
library(ggplot2)
library(readr)
library(lubridate)
# ğŸ“Œ Criar uma nova sessÃ£o do Chromote
b <- ChromoteSession$new()
# ğŸ“Œ FunÃ§Ã£o para capturar vagas no LinkedIn com filtro de perÃ­odo e descriÃ§Ã£o da vaga
scrape_linkedin_jobs <- function(keyword, pages = 3, periodo = "r5184000") {
print(paste("Iniciando raspagem para:", keyword))  # Mensagem de status
base_url <- "https://www.linkedin.com/jobs/search/?"
query <- paste0("keywords=", URLencode(keyword),
"&location=Brazil",
"&f_TPR=", periodo)
full_url <- paste0(base_url, query)
b$Page$navigate(full_url)
Sys.sleep(5)  # Tempo para carregar a pÃ¡gina
job_list <- list()
for (i in 1:pages) {
print(paste("Processando pÃ¡gina", i, "de", pages))  # Mensagem de status
page_source <- b$Runtime$evaluate("document.documentElement.outerHTML")$result$value
html <- read_html(page_source)
titles <- html %>% html_nodes(".base-search-card__title") %>% html_text(trim = TRUE)
companies <- html %>% html_nodes(".base-search-card__subtitle") %>% html_text(trim = TRUE)
locations <- html %>% html_nodes(".job-search-card__location") %>% html_text(trim = TRUE)
links <- html %>% html_nodes(".base-card__full-link") %>% html_attr("href")
dates <- html %>% html_nodes(".job-search-card__listdate") %>% html_text(trim = TRUE)
# Garantir que todas as colunas tenham o mesmo tamanho
max_length <- max(length(titles), length(companies), length(locations), length(links), length(dates))
titles <- c(titles, rep(NA, max_length - length(titles)))
companies <- c(companies, rep(NA, max_length - length(companies)))
locations <- c(locations, rep(NA, max_length - length(locations)))
links <- c(links, rep(NA, max_length - length(links)))
dates <- c(dates, rep(NA, max_length - length(dates)))
# Converter texto das datas para formato legÃ­vel
dates <- dates %>%
str_replace_all("hÃ¡ ", "") %>%
str_replace_all("dia", "dias") %>%
str_replace_all("um", "1") %>%
str_extract("\\d+") %>%
as.numeric() %>%
replace_na(0) %>%
{Sys.Date() - .}  # ğŸ”¹ Subtrai os dias para estimar a data real da postagem
# ğŸ”¹ Coletar descriÃ§Ã£o de cada vaga
descriptions <- map_chr(links, function(url) {
if (!is.na(url)) {
print(paste("Coletando descriÃ§Ã£o da vaga em:", url))  # Mensagem de status
b$Page$navigate(url)
Sys.sleep(3)  # Esperar carregamento
page_source <- b$Runtime$evaluate("document.documentElement.outerHTML")$result$value
job_html <- read_html(page_source)
# Extrair descriÃ§Ã£o da vaga
job_html %>%
html_nodes(".description__text") %>%
html_text(trim = TRUE) %>%
paste(collapse = " ")  # Concatenar caso haja mÃºltiplos elementos
} else {
NA
}
})
# Garantir que descriptions tenha o mesmo tamanho
descriptions <- c(descriptions, rep(NA, max_length - length(descriptions)))
# Criar dataframe e adicionar Ã  lista
job_data <- tibble(
Titulo = titles,
Empresa = companies,
Localizacao = locations,
Link = links,
Data_Publicacao = dates,
Descricao = descriptions
)
job_list <- append(job_list, list(job_data))
}
return(bind_rows(job_list))
}
# ğŸ“Œ Lista de palavras-chave para busca
keywords <- c("Data Scientist", "Data Science", "CiÃªncia de Dados",
"Data Engineering", "Engenharia de Dados", "Engenheiro de Dados")
# ğŸ“Œ Coletar vagas para cada palavra-chave (Ãšltimos 60 dias)
vagas_total <- map_dfr(keywords, ~scrape_linkedin_jobs(.x, pages = 3, periodo = "r5184000"))
# ğŸ“Œ Carregar bibliotecas necessÃ¡rias
library(chromote)
library(rvest)
library(dplyr)
library(stringr)
library(tidyverse)
library(parsedate)
library(ggplot2)
library(readr)
library(lubridate)
library(crayon)  # Adicionando pacote para cores no console
# ğŸ“Œ Criar uma nova sessÃ£o do Chromote
b <- ChromoteSession$new()
# ğŸ“Œ FunÃ§Ã£o para capturar vagas no LinkedIn com filtro de perÃ­odo e descriÃ§Ã£o da vaga
scrape_linkedin_jobs <- function(keyword, pages = 3, periodo = "r5184000") {
message(blue$bold(paste("Iniciando raspagem para:", keyword)))  # Mensagem colorida de status
base_url <- "https://www.linkedin.com/jobs/search/?"
query <- paste0("keywords=", URLencode(keyword),
"&location=Brazil",
"&f_TPR=", periodo)
full_url <- paste0(base_url, query)
b$Page$navigate(full_url)
Sys.sleep(5)  # Tempo para carregar a pÃ¡gina
job_list <- list()
for (i in 1:pages) {
message(blue$bold(paste("Processando pÃ¡gina", i, "de", pages)))  # Mensagem colorida
page_source <- b$Runtime$evaluate("document.documentElement.outerHTML")$result$value
html <- read_html(page_source)
titles <- html %>% html_nodes(".base-search-card__title") %>% html_text(trim = TRUE)
companies <- html %>% html_nodes(".base-search-card__subtitle") %>% html_text(trim = TRUE)
locations <- html %>% html_nodes(".job-search-card__location") %>% html_text(trim = TRUE)
links <- html %>% html_nodes(".base-card__full-link") %>% html_attr("href")
dates <- html %>% html_nodes(".job-search-card__listdate") %>% html_text(trim = TRUE)
# Garantir que todas as colunas tenham o mesmo tamanho
max_length <- max(length(titles), length(companies), length(locations), length(links), length(dates))
titles <- c(titles, rep(NA, max_length - length(titles)))
companies <- c(companies, rep(NA, max_length - length(companies)))
locations <- c(locations, rep(NA, max_length - length(locations)))
links <- c(links, rep(NA, max_length - length(links)))
dates <- c(dates, rep(NA, max_length - length(dates)))
# Converter texto das datas para formato legÃ­vel
dates <- dates %>%
str_replace_all("hÃ¡ ", "") %>%
str_replace_all("dia", "dias") %>%
str_replace_all("um", "1") %>%
str_extract("\\d+") %>%
as.numeric() %>%
replace_na(0) %>%
{Sys.Date() - .}
# ğŸ”¹ Coletar descriÃ§Ã£o de cada vaga
descriptions <- map_chr(links, function(url) {
if (!is.na(url)) {
message(green$bold(paste("Coletando descriÃ§Ã£o da vaga em:", url)))  # Mensagem colorida
b$Page$navigate(url)
Sys.sleep(3)
page_source <- b$Runtime$evaluate("document.documentElement.outerHTML")$result$value
job_html <- read_html(page_source)
job_html %>%
html_nodes(".description__text") %>%
html_text(trim = TRUE) %>%
paste(collapse = " ")
} else {
NA
}
})
descriptions <- c(descriptions, rep(NA, max_length - length(descriptions)))
# Criar dataframe e adicionar Ã  lista
job_data <- tibble(
Titulo = titles,
Empresa = companies,
Localizacao = locations,
Link = links,
Data_Publicacao = dates,
Descricao = descriptions
)
job_list <- append(job_list, list(job_data))
}
return(bind_rows(job_list))
}
# ğŸ“Œ Lista de palavras-chave para busca
keywords <- c("Data Scientist", "Data Science", "CiÃªncia de Dados",
"Data Engineering", "Engenharia de Dados", "Engenheiro de Dados")
# ğŸ“Œ Coletar vagas para cada palavra-chave (Ãšltimos 60 dias)
vagas_total <- map_dfr(keywords, ~scrape_linkedin_jobs(.x, pages = 3, periodo = "r5184000"))
# ğŸ“Œ Salvar os dados coletados
write_csv(vagas_total, "vagas_linkedin.csv")
message(green$bold("âœ… Dados salvos em 'vagas_linkedin.csv'"))
# ğŸ“Œ Fechar sessÃ£o Chromote
b$close()
# ğŸ“Œ Verificar se o arquivo foi salvo corretamente
if (file.exists("vagas_linkedin.csv")) {
message(green$bold("âœ… O arquivo 'vagas_linkedin.csv' foi gerado com sucesso!"))
data_linkedin <- read_csv("vagas_linkedin.csv")
} else {
stop(red$bold("âŒ ERRO: O arquivo 'vagas_linkedin.csv' nÃ£o foi encontrado!"))
}
# ğŸ“Œ Carregar bibliotecas necessÃ¡rias
library(chromote)
library(rvest)
library(dplyr)
library(stringr)
library(tidyverse)
library(parsedate)
library(ggplot2)
library(readr)
library(lubridate)
library(crayon)  # Adicionando pacote para cores no console
# ğŸ“Œ Criar uma nova sessÃ£o do Chromote
b <- ChromoteSession$new()
# ğŸ“Œ FunÃ§Ã£o para capturar vagas no LinkedIn com filtro de perÃ­odo e descriÃ§Ã£o da vaga
scrape_linkedin_jobs <- function(keyword, pages = 5, periodo = "r5184000") {
message(blue$bold(paste("Iniciando raspagem para:", keyword)))  # Mensagem colorida de status
base_url <- "https://www.linkedin.com/jobs/search/?"
query <- paste0("keywords=", URLencode(keyword),
"&location=Brazil",
"&f_TPR=", periodo)
full_url <- paste0(base_url, query)
b$Page$navigate(full_url)
Sys.sleep(5)  # Tempo para carregar a pÃ¡gina
job_list <- list()
for (i in 1:pages) {
message(blue$bold(paste("Processando pÃ¡gina", i, "de", pages)))  # Mensagem colorida
page_source <- b$Runtime$evaluate("document.documentElement.outerHTML")$result$value
html <- read_html(page_source)
titles <- html %>% html_nodes(".base-search-card__title") %>% html_text(trim = TRUE)
companies <- html %>% html_nodes(".base-search-card__subtitle") %>% html_text(trim = TRUE)
locations <- html %>% html_nodes(".job-search-card__location") %>% html_text(trim = TRUE)
links <- html %>% html_nodes(".base-card__full-link") %>% html_attr("href")
dates <- html %>% html_nodes(".job-search-card__listdate") %>% html_text(trim = TRUE)
# Garantir que todas as colunas tenham o mesmo tamanho
max_length <- max(length(titles), length(companies), length(locations), length(links), length(dates))
titles <- c(titles, rep(NA, max_length - length(titles)))
companies <- c(companies, rep(NA, max_length - length(companies)))
locations <- c(locations, rep(NA, max_length - length(locations)))
links <- c(links, rep(NA, max_length - length(links)))
dates <- c(dates, rep(NA, max_length - length(dates)))
# Converter texto das datas para formato legÃ­vel
dates <- dates %>%
str_replace_all("hÃ¡ ", "") %>%
str_replace_all("dia", "dias") %>%
str_replace_all("um", "1") %>%
str_extract("\\d+") %>%
as.numeric() %>%
replace_na(0) %>%
{Sys.Date() - .}
# ğŸ”¹ Coletar descriÃ§Ã£o de cada vaga
descriptions <- map_chr(links, function(url) {
if (!is.na(url)) {
message(green$bold(paste("Coletando descriÃ§Ã£o da vaga em:", url)))  # Mensagem colorida
b$Page$navigate(url)
Sys.sleep(3)
page_source <- b$Runtime$evaluate("document.documentElement.outerHTML")$result$value
job_html <- read_html(page_source)
job_html %>%
html_nodes(".description__text") %>%
html_text(trim = TRUE) %>%
paste(collapse = " ")
} else {
NA
}
})
descriptions <- c(descriptions, rep(NA, max_length - length(descriptions)))
# Criar dataframe e adicionar Ã  lista
job_data <- tibble(
Titulo = titles,
Empresa = companies,
Localizacao = locations,
Link = links,
Data_Publicacao = dates,
Descricao = descriptions
)
job_list <- append(job_list, list(job_data))
}
return(bind_rows(job_list))
}
# ğŸ“Œ Lista de palavras-chave para busca
keywords <- c("Data Scientist", "Data Science", "CiÃªncia de Dados",
"Data Engineering", "Engenharia de Dados", "Engenheiro de Dados")
# ğŸ“Œ Coletar vagas para cada palavra-chave (Ãšltimos 60 dias)
vagas_total <- map_dfr(keywords, ~scrape_linkedin_jobs(.x, pages = 3, periodo = "r5184000"))
# ğŸ“Œ Carregar bibliotecas necessÃ¡rias
library(chromote)
library(rvest)
library(dplyr)
library(stringr)
library(tidyverse)
library(parsedate)
library(ggplot2)
library(readr)
library(lubridate)
library(crayon)  # Adicionando pacote para cores no console
# ğŸ“Œ Criar uma nova sessÃ£o do Chromote
b <- ChromoteSession$new()
# ğŸ“Œ FunÃ§Ã£o para capturar vagas no LinkedIn com filtro de perÃ­odo e descriÃ§Ã£o da vaga
scrape_linkedin_jobs <- function(keyword, pages = 3, periodo = "r5184000") {
message(blue$bold(paste("Iniciando raspagem para:", keyword)))  # Mensagem colorida de status
base_url <- "https://www.linkedin.com/jobs/search/?"
query <- paste0("keywords=", URLencode(keyword),
"&location=Brazil",
"&f_TPR=", periodo)
full_url <- paste0(base_url, query)
b$Page$navigate(full_url)
Sys.sleep(5)  # Tempo para carregar a pÃ¡gina
job_list <- list()
for (i in 1:pages) {
message(blue$bold(paste("Processando pÃ¡gina", i, "de", pages)))  # Mensagem colorida
page_source <- b$Runtime$evaluate("document.documentElement.outerHTML")$result$value
html <- read_html(page_source)
titles <- html %>% html_nodes(".base-search-card__title") %>% html_text(trim = TRUE)
companies <- html %>% html_nodes(".base-search-card__subtitle") %>% html_text(trim = TRUE)
locations <- html %>% html_nodes(".job-search-card__location") %>% html_text(trim = TRUE)
links <- html %>% html_nodes(".base-card__full-link") %>% html_attr("href")
dates <- html %>% html_nodes(".job-search-card__listdate") %>% html_text(trim = TRUE)
# Garantir que todas as colunas tenham o mesmo tamanho
max_length <- max(length(titles), length(companies), length(locations), length(links), length(dates))
titles <- c(titles, rep(NA, max_length - length(titles)))
companies <- c(companies, rep(NA, max_length - length(companies)))
locations <- c(locations, rep(NA, max_length - length(locations)))
links <- c(links, rep(NA, max_length - length(links)))
dates <- c(dates, rep(NA, max_length - length(dates)))
# Converter texto das datas para formato legÃ­vel
dates <- dates %>%
str_replace_all("hÃ¡ ", "") %>%
str_replace_all("dia", "dias") %>%
str_replace_all("um", "1") %>%
str_extract("\\d+") %>%
as.numeric() %>%
replace_na(0) %>%
{Sys.Date() - .}
# ğŸ”¹ Coletar descriÃ§Ã£o de cada vaga
descriptions <- map_chr(links, function(url) {
if (!is.na(url)) {
message(green$bold(paste("Coletando descriÃ§Ã£o da vaga em:", url)))  # Mensagem colorida
b$Page$navigate(url)
Sys.sleep(3)
page_source <- b$Runtime$evaluate("document.documentElement.outerHTML")$result$value
job_html <- read_html(page_source)
job_html %>%
html_nodes(".description__text") %>%
html_text(trim = TRUE) %>%
paste(collapse = " ")
} else {
NA
}
})
descriptions <- c(descriptions, rep(NA, max_length - length(descriptions)))
# Criar dataframe e adicionar Ã  lista
job_data <- tibble(
Titulo = titles,
Empresa = companies,
Localizacao = locations,
Link = links,
Data_Publicacao = dates,
Descricao = descriptions
)
job_list <- append(job_list, list(job_data))
}
return(bind_rows(job_list))
}
# ğŸ“Œ Lista de palavras-chave para busca
keywords <- c("Data Scientist", "Data Science", "CiÃªncia de Dados",
"Data Engineering", "Engenharia de Dados", "Engenheiro de Dados")
# ğŸ“Œ Coletar vagas para cada palavra-chave (Ãšltimos 60 dias)
vagas_total <- map_dfr(keywords, ~scrape_linkedin_jobs(.x, pages = 5, periodo = "r5184000"))
# ğŸ“Œ Salvar os dados coletados
write_csv(vagas_total, "vagas_linkedin.csv")
message(green$bold("âœ… Dados salvos em 'vagas_linkedin.csv'"))
# ğŸ“Œ Fechar sessÃ£o Chromote
b$close()
# ğŸ“Œ Verificar se o arquivo foi salvo corretamente
if (file.exists("vagas_linkedin.csv")) {
message(green$bold("âœ… O arquivo 'vagas_linkedin.csv' foi gerado com sucesso!"))
data_linkedin <- read_csv("vagas_linkedin.csv")
} else {
stop(red$bold("âŒ ERRO: O arquivo 'vagas_linkedin.csv' nÃ£o foi encontrado!"))
}
View(vagas_total)
View(data_linkedin)
View(vagas_total)
View(data_linkedin)
# ğŸ“Œ Carregar bibliotecas necessÃ¡rias
library(dplyr)
library(stringr)
library(ggplot2)
library(readr)
library(tidyr)
# ğŸ“Œ Carregar os dados do CSV
df <- read_csv("vagas_linkedin.csv")
# ğŸ“Œ Lista de ferramentas para buscar nas descriÃ§Ãµes
ferramentas <- c(
"Python", "Linguagem R", "R Language", "SQL", "Scala", "Java", "Julia", "C++", "C#", "Go", "Rust",
"Pandas", "NumPy", "Scikit-Learn", "TensorFlow", "PyTorch", "Keras",
"Matplotlib", "Seaborn", "NLTK", "OpenCV", "XGBoost", "LightGBM",
"Hugging Face", "FastAPI", "Streamlit", "Plotly", "DVC", "MLflow",
"AWS", "Azure", "Google Cloud", "GCP", "BigQuery", "Snowflake", "Databricks",
"Oracle Cloud", "IBM Cloud", "Alibaba Cloud", "DigitalOcean", "Linode", "Heroku",
"Firebase", "Cloud Functions", "Lambda", "Cloud Run", "EC2", "S3", "IAM",
"Redshift", "Cloud SQL", "Cloud Storage", "Vertex AI", "SageMaker", "Dataflow",
"Glue", "Cloud Composer",
"Power BI", "Tableau", "Looker", "Google Data Studio", "Qlik Sense", "QlikView",
"Metabase", "Superset", "Grafana", "Sisense", "Domo", "Mode Analytics"
)
# ğŸ“Œ Criar uma tabela para armazenar as contagens das ferramentas
df_ferramentas <- tibble(Ferramenta = ferramentas, Quantidade = 0)
# ğŸ“Œ Remover NAs da coluna "Descricao"
df <- df %>% mutate(Descricao = ifelse(is.na(Descricao), "", tolower(Descricao)))
# ğŸ“Œ Aplicar Regex para contar menÃ§Ãµes das ferramentas na coluna DescriÃ§Ã£o
for (ferramenta in ferramentas) {
df_ferramentas <- df_ferramentas %>%
mutate(Quantidade = ifelse(Ferramenta == ferramenta,
sum(str_detect(df$Descricao, regex(ferramenta, ignore_case = TRUE))),
Quantidade))
}
# ğŸ“Œ Selecionar as 15 ferramentas mais mencionadas
df_top15 <- df_ferramentas %>%
arrange(desc(Quantidade)) %>%
head(15)
# ğŸ“Œ Criar um grÃ¡fico de barras com as 15 ferramentas mais mencionadas
ggplot(df_top15, aes(x = reorder(Ferramenta, Quantidade), y = Quantidade)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() +
labs(title = "Top 15 Ferramentas mais Requisitadas em Vagas de Data Science",
x = "Ferramenta",
y = "NÃºmero de MenÃ§Ãµes") +
theme_minimal()
